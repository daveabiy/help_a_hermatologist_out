{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_model = False\n",
    "from codecs import ignore_errors\n",
    "import os\n",
    "from time import sleep\n",
    "from glob import glob\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import ntpath\n",
    "\n",
    "import numpy as np\n",
    "from imageio import imread\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix, matthews_corrcoef, classification_report,confusion_matrix, accuracy_score, balanced_accuracy_score, cohen_kappa_score, f1_score,  precision_score, recall_score\n",
    "from skimage import io as io\n",
    "from skimage.util import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "import data_processing as dp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> import metadata </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = {\n",
    "        \"Ace_20\": \"/beegfs/desy/user/hailudaw/challenge/Datasets/Acevedo_20\", # Acevedo_20 Dataset\n",
    "        \"Mat_19\": \"/beegfs/desy/user/hailudaw/challenge/Datasets/Matek_19\", # Matek_19 Dataset\n",
    "        \"Ace_20_noisy\": \"/beegfs/desy/user/hailudaw/challenge/Datasets/Acevedo_20_noisy\", # Acevedo_20 Dataset\n",
    "        \"Mat_19_noisy\": \"/beegfs/desy/user/hailudaw/challenge/Datasets/Matek_19_noisy\", # Matek_19 Dataset\n",
    "        \"WBC1\": \"/beegfs/desy/user/hailudaw/challenge/Datasets/WBC1\", # WBC1 dataset\n",
    "        \"WBC2\": \"/beegfs/desy/user/hailudaw/challenge/Datasets/WBC2\"\n",
    "    }\n",
    "\n",
    "label_map_all = {\n",
    "        'basophil': 0,\n",
    "        'eosinophil': 1,\n",
    "        'erythroblast': 2,\n",
    "        'myeloblast' : 3,\n",
    "        'promyelocyte': 4,\n",
    "        'myelocyte': 5,\n",
    "        'metamyelocyte': 6,\n",
    "        'neutrophil_banded': 7,\n",
    "        'neutrophil_segmented': 8,\n",
    "        'monocyte': 9,\n",
    "        'lymphocyte_typical': 10\n",
    "    }\n",
    "\n",
    "label_map_reverse = {\n",
    "        0: 'basophil',\n",
    "        1: 'eosinophil',\n",
    "        2: 'erythroblast',\n",
    "        3: 'myeloblast',\n",
    "        4: 'promyelocyte',\n",
    "        5: 'myelocyte',\n",
    "        6: 'metamyelocyte',\n",
    "        7: 'neutrophil_banded',\n",
    "        8: 'neutrophil_segmented',\n",
    "        9: 'monocyte',\n",
    "        10: 'lymphocyte_typical'\n",
    "    }\n",
    "\n",
    "# The unlabeled WBC dataset gets the classname 'Data-Val' for every image\n",
    "\n",
    "label_map_pred = {\n",
    "        'DATA-VAL': 0\n",
    "    }\n",
    "label_map_after = {\n",
    "        'DATA-TEST': 0\n",
    "    }\n",
    "\n",
    "# ## Data loading\n",
    "# We use pandas dataframes to systematically order and later load the data:\n",
    "\n",
    "savepaths=['metadata.csv', 'metadata_all.csv', 'metadata_with_noisy.csv', 'metadata2.csv', 'metadata3.csv'] # path where the created dataframe will be stored\n",
    "savepath = savepaths[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(savepath)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ace_metadata=metadata.loc[metadata['dataset']=='Ace_20'].reset_index(drop = True)\n",
    "ace_metadata_noisy = metadata.loc[metadata['dataset']=='Ace_20_noisy'].reset_index(drop = True)\n",
    "mat_metadata=metadata.loc[metadata['dataset']=='Mat_19'].reset_index(drop = True)\n",
    "mat_metadata_noisy = metadata.loc[metadata['dataset']=='Mat_19_noisy'].reset_index(drop = True)\n",
    "wbc_metadata=metadata.loc[metadata['dataset']=='WBC1'].reset_index(drop = True)\n",
    "wbc2_metadata=metadata.loc[metadata['dataset']=='WBC2'].reset_index(drop = True)\n",
    "ace_metadata_noisy_drop_basophil = ace_metadata_noisy[ace_metadata_noisy['label'] != 'basophil'].reset_index(drop = True)\n",
    "mat_metadata_noisy_drop_neutrophil_segmented = mat_metadata_noisy[mat_metadata_noisy['label'] != 'neutrophil_segmented'].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_data(metadata=wbc2_metadata):\n",
    "    outputdata =wbc_metadata.drop(columns=['file','label', 'dataset', 'set', 'mean1', 'mean2', 'mean3', 'std1', 'std2', 'std3', 'max1', 'max2', 'max3', 'min1', 'min2', 'min3'])\n",
    "    outputdata['Label']=None\n",
    "    outputdata['LabelID']=None\n",
    "    for i in range(len(outputdata)):\n",
    "        outputdata['LabelID'].loc[i]=random.randint(0, 10) #for the 10 possible classes\n",
    "        outputdata['Label'].loc[i]=label_map_reverse[outputdata['LabelID'].loc[i]]\n",
    "    outputdata.to_csv('submission.csv', index=False)\n",
    "    return outputdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdata1 = output_data(wbc_metadata)\n",
    "outputdata2 = output_data(wbc2_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_metadata=metadata\n",
    "source_domains=['Ace_20', 'Mat_19', 'Ace_20_noisy', 'Mat_19_noisy']\n",
    "source_index = example_metadata.dataset.isin(source_domains)\n",
    "example_metadata = example_metadata.loc[source_index,:].copy().reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_metadata_noisy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fraction=0.2 #of the whole dataset\n",
    "val_fraction=0.125 #of 0.8 of the dataset (corresponds to 0.1 of the whole set)\n",
    "train_index, test_index, train_label, test_label = train_test_split(\n",
    "    example_metadata.index,\n",
    "    example_metadata.label + \"_\" + example_metadata.dataset,\n",
    "    test_size=test_fraction,\n",
    "    random_state=0, \n",
    "    shuffle=True,\n",
    "    stratify=example_metadata.label\n",
    "    )\n",
    "example_metadata.loc[test_index, 'set']='test'\n",
    "train_val_metadata=example_metadata.loc[train_index]\n",
    "\n",
    "train_index, val_index, train_label, val_label = train_test_split(\n",
    "    train_val_metadata.index,\n",
    "    train_val_metadata.label + \"_\" + train_val_metadata.dataset,\n",
    "    test_size=val_fraction,\n",
    "    random_state=0, \n",
    "    shuffle=True, \n",
    "    stratify=train_val_metadata.label\n",
    "    )\n",
    "example_metadata.loc[val_index, 'set']='val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=len(example_metadata.loc[example_metadata['set'] == 'train'])\n",
    "val_size=len(example_metadata.loc[example_metadata['set'] == 'val'])\n",
    "test_size=len(example_metadata.loc[example_metadata['set'] == 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_metadata.loc[:, 'set'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_generator as dg\n",
    "resize=224 #image pixel size\n",
    "number_workers=3\n",
    "\n",
    "random_crop_scale=(0.8, 1.0)\n",
    "random_crop_ratio=(0.8, 1.2)\n",
    "\n",
    "mean=[0.485, 0.456, 0.406] #values from imagenet\n",
    "std=[0.229, 0.224, 0.225] #values from imagenet\n",
    "\n",
    "bs=25 #batchsize\n",
    "normalization = torchvision.transforms.Normalize(mean,std)\n",
    "\n",
    "train_transform = transforms.Compose([ \n",
    "        normalization,\n",
    "        transforms.RandomResizedCrop(resize, scale=random_crop_scale, ratio=random_crop_ratio),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip()\n",
    "])\n",
    "val_transform = transforms.Compose([ \n",
    "        normalization,\n",
    "        transforms.Resize(resize)])\n",
    "\n",
    "test_transform = transforms.Compose([ \n",
    "        normalization,\n",
    "        transforms.Resize(resize)])\n",
    "\n",
    "\n",
    "train_dataset = dg.DatasetGenerator(example_metadata.loc[train_index,:], \n",
    "                                 reshape_size=resize, \n",
    "                                 dataset = source_domains,\n",
    "                                 label_map=label_map_all, \n",
    "                                 transform = train_transform,\n",
    "                                 )\n",
    "val_dataset = dg.DatasetGenerator(example_metadata.loc[val_index,:], \n",
    "                                 reshape_size=resize, \n",
    "                                 dataset = source_domains,\n",
    "                                 label_map=label_map_all, \n",
    "                                 transform = val_transform,\n",
    "                                 )\n",
    "\n",
    "test_dataset = dg.DatasetGenerator(example_metadata.loc[test_index,:], \n",
    "                                 reshape_size=resize, \n",
    "                                 dataset = source_domains,\n",
    "                                 label_map=label_map_all, \n",
    "                                 transform = test_transform,\n",
    "                                 )\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=bs, shuffle=True, num_workers=number_workers)\n",
    "valid_loader = DataLoader(\n",
    "    val_dataset, batch_size=bs, shuffle=True, num_workers=number_workers)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=bs, shuffle=False, num_workers=number_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chose an Architecture and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20 # max number of epochs\n",
    "lr=0.003 # learning rate\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_classes = len(label_map_all)\n",
    "architectures = ['resnet18', 'resnet50', 'resnet152']\n",
    "arch = architectures[1]   #set the architecture type here\n",
    "import torchvision.models as models\n",
    "\n",
    "if (arch == 'resnet18'):\n",
    "    from torchvision.models import resnet18\n",
    "    model = resnet18(pretrained=True)\n",
    "    model_save_path='model' #path where model with best f1_macro should be stored\n",
    "elif (arch == 'resnet50'):\n",
    "    from torchvision.models import resnet50\n",
    "    model = resnet50(pretrained=True)\n",
    "    model_save_path='model50' #path where model with best f1_macro should be stored\n",
    "    \n",
    "else:# (arch == 'resnet152'):\n",
    "    from torchvision.models import resnet152\n",
    "    model = resnet152(pretrained=True)\n",
    "    model_save_path='model152' #path where model with best f1_macro should be stored\n",
    "if update_model == True:\n",
    "    model = torch.load(model_save_path)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "model = nn.DataParallel(model) \n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "# #running variables\n",
    "epoch=0\n",
    "update_frequency=5 # number of batches before viewed acc and loss get updated\n",
    "counter=0 #counts batches\n",
    "f1_macro_best=0 #minimum f1_macro_score of the validation set for the first model to be saved\n",
    "loss_running=0\n",
    "acc_running=0\n",
    "val_batches=0\n",
    "\n",
    "y_pred=torch.tensor([], dtype=int)\n",
    "y_true=torch.tensor([], dtype=int)\n",
    "y_pred=y_pred.to(device)\n",
    "y_true=y_true.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(i = 0, data = metadata, counter = counter, tepoch = tqdm(train_loader)):\n",
    "    tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "    counter+=1\n",
    "    print(counter)\n",
    "\n",
    "    x, y = data\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(x)\n",
    "    loss = criterion(out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    logits = torch.softmax(out.detach(), dim=1)\n",
    "    predictions = logits.argmax(dim=1)\n",
    "    acc = accuracy_score(y.cpu(), predictions.cpu())\n",
    "    \n",
    "    if counter >= update_frequency:\n",
    "        tepoch.set_postfix(loss=loss.item(), accuracy=acc.item())\n",
    "        counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(0, epochs):\n",
    "    #training\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm(train_loader) as tepoch:  \n",
    "        for i, data in enumerate(tepoch):\n",
    "            tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "            counter+=1\n",
    "\n",
    "            x, y = data\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            logits = torch.softmax(out.detach(), dim=1)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            acc = accuracy_score(y.cpu(), predictions.cpu())\n",
    "            \n",
    "            if counter >= update_frequency:\n",
    "                tepoch.set_postfix(loss=loss.item(), accuracy=acc.item())\n",
    "                counter=0\n",
    "                \n",
    "    #validation       \n",
    "    model.eval()\n",
    "    with tqdm(valid_loader) as vepoch: \n",
    "        for i, data in enumerate(vepoch):\n",
    "            vepoch.set_description(f\"Validation {epoch+1}\")\n",
    "    \n",
    "            x, y = data\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            \n",
    "            logits = torch.softmax(out.detach(), dim=1)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            y_pred=torch.cat((y_pred, predictions), 0)\n",
    "            y_true=torch.cat((y_true, y), 0)\n",
    "            \n",
    "            acc = accuracy_score(y_true.cpu(), y_pred.cpu())\n",
    "            \n",
    "            loss_running+=(loss.item()*len(y))\n",
    "            acc_running+=(acc.item()*len(y))\n",
    "            val_batches+=len(y)\n",
    "            loss_mean=loss_running/val_batches\n",
    "            acc_mean=acc_running/val_batches\n",
    "            \n",
    "            vepoch.set_postfix(loss=loss_mean, accuracy=acc_mean)\n",
    "            \n",
    "        f1_micro=f1_score(y_true.cpu(), y_pred.cpu(), average='micro')\n",
    "        f1_macro=f1_score(y_true.cpu(), y_pred.cpu(), average='macro')\n",
    "        print(f'f1_micro: {f1_micro}, f1_macro: {f1_macro}')  \n",
    "        if f1_macro > f1_macro_best:\n",
    "            f1_macro_best=f1_macro\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print('model saved')\n",
    "        \n",
    "        #reseting running variables\n",
    "        loss_running=0\n",
    "        acc_running=0\n",
    "        val_batches=0\n",
    "            \n",
    "        y_pred=torch.tensor([], dtype=int)\n",
    "        y_true=torch.tensor([], dtype=int)\n",
    "        y_pred=y_pred.to(device)\n",
    "        y_true=y_true.to(device)\n",
    "            \n",
    "        \n",
    "    \n",
    "print('Finished Training')\n",
    "\n",
    "#loading the model with the highest validation accuracy\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3178f61136cc71cf7bfb960c3038ac41089286ea2dd8757f89d6514789c9199a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
