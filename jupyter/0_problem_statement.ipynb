{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Challenge: [Help a Hematologist out!](https://helmholtz-data-challenges.de/web/challenges/challenge-page/93/overview) \n",
    "*** \n",
    "<b> Group: </b> \n",
    "> #      $ BLAMAD $  \n",
    "<b> members </b> \n",
    "> Bashir K., \n",
    "> Lea G., \n",
    "> Ankita N., \n",
    "> Martin B., \n",
    "> Arnab M., \n",
    "> Dawit H. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](https://github.com/christinab12/Data-challenge-logo/blob/main/logo.jpg?raw=true)\n",
    "\n",
    "## Getting started\n",
    "\n",
    "\n",
    "This notebook is a short summary for getting started with the challenge ( found [here](https://helmholtz-data-challenges.de/web/challenges/challenge-page/93/overview)  ). Below you can find how to download the dataset and also the different labels along with exploring and analyzing the input and output data of the challenge, running a baseline model and creating a submission file to upload to the leaderboard.\n",
    "\n",
    "***\n",
    "\n",
    "<b>dataset:</b>\n",
    "\n",
    "Three datasets, each constituting a different domain, will be used for this challenge:\n",
    "> 1. The Acevedo_20 dataset with labels\n",
    "> 2. The Matek_19 dataset with labels\n",
    "> 3. The WBC dataset <b> without labels </b> (Used for domain adaptation and performance measurement)\n",
    "\n",
    "The Acevedo_20 and Matek_19 datasets are labeled and should be used to train the model for the domain generalization task.\n",
    "A small subpart of the WBC dataset, WBC1, will be downloadable from the beginning of the challenge. It is unlabeled and should be used for evaluation and domain adaptation techniques.\n",
    "\n",
    "A second similar subpart of the WBC dataset, WBC2, will become available for download during phase 2 of the challenge, i.e. on the last day, 24 hours before submissions close.\n",
    "\n",
    "***\n",
    "<b>Goal: </b> \n",
    "\n",
    "The challenge here is in transfer learning, <b> precisely domain generalization (DG) and domain adaptation (DA) </b> techniques. The focus lies on using deep neural networks to classify single white blood cell images obtained from peripheral blood smears.\n",
    "<b> Tthe goal of this challenge is to achieve a high performance, especially a high f1 macro score, on the WBC2 dataset. </b>\n",
    "\n",
    "***\n",
    "<b>Notes: </b>\n",
    "\n",
    "This challenge wants to motivate research in domain generalization and adaptation techniques:\n",
    "\n",
    "To make actual use of deep learning in the medical routine, it is important that the techniques can be used in realistic cases. If a peripheral blood smear is acquired from a patient and classified by a neural network, it is important that this works reliably. But the patientâ€™s blood smear might very likely vary compared to the image domains used as training data of the network, resulting in not trustable results. To overcome this obstacle and build robust domain-invariant classifiers research in domain generalization and adaptation is needed.\n",
    "\n",
    "***\n",
    "<b>f1_score: </b>\n",
    "[wikepedia](https://en.wikipedia.org/wiki/F-score)\n",
    "\n",
    "> sklearn.metrics.f1_score(y_true, y_pred, *, labels=None, pos_label=1,<b> average='macro' </b>, sample_weight=None, zero_division='warn')\n",
    "\n",
    "The formula can be see in [click here for the code](https://github.com/scikit-learn/scikit-learn/blob/36958fb24/sklearn/metrics/_classification.py#L1001) and is given as\n",
    "\n",
    "> <g> F1 = 2 * (precision * recall) / (precision + recall) </g>\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Donwloading the data\n",
    "***\n",
    " Uncomment the code below to download the dataset. Makesure you adjust the path according to where you download it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget --user YraZEdrHytaCSza --password BgZL3j8DT4 https://hmgubox2.helmholtz-muenchen.de/public.php/webdav/Acevedo_20.zip -O Acevedo_20.zip #(230M) [application/zip]\n",
    "# !wget --user YraZEdrHytaCSza --password BgZL3j8DT4 https://hmgubox2.helmholtz-muenchen.de/public.php/webdav/Matek_19.zip -O Matek_19.zip #(5.7G) [application/zip]\n",
    "# !wget --user YraZEdrHytaCSza --password BgZL3j8DT4 https://hmgubox2.helmholtz-muenchen.de/public.php/webdav/WBC1.zip -O WBC1.zip #(357M) [application/zip]\n",
    "# !wget --user YraZEdrHytaCSza --password BgZL3j8DT4 https://hmgubox2.helmholtz-muenchen.de/public.php/webdav/val_dummy.csv -O val_dummy.csv #44834 (44K) [text/csv]\n",
    "# !wget --user YraZEdrHytaCSza --password BgZL3j8DT4 https://hmgubox2.helmholtz-muenchen.de/public.php/webdav/metadata2.csv -O metadata2.csv #2019059 (1.9M) [text/csv]\n",
    "# print('download complete') \n",
    "\n",
    "# import shutil\n",
    "\n",
    "# shutil.unpack_archive('Acevedo_20.zip', 'Datasets/Acevedo_20')\n",
    "# shutil.unpack_archive('Matek_19.zip', 'Datasets/Matek_19')\n",
    "# shutil.unpack_archive('WBC1.zip', 'Datasets/WBC1')\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> datapath </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = {\n",
    "        \"Ace_20\": \"/beegfs/desy/user/hailudaw/challenge/Datasets/Acevedo_20\", # Acevedo_20 Dataset\n",
    "        \"Mat_19\": \"/beegfs/desy/user/hailudaw/challenge/Datasets/Matek_19\", # Matek_19 Dataset\n",
    "        \"WBC1\": \"/beegfs/desy/user/hailudaw/challenge/Datasets/WBC1\" # WBC1 dataset\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> labels </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common classes of the datasets and their labels: \n",
    "# Highly underrepresented classes like atypical lymphocytes and smudge cells were left out.\n",
    "\n",
    "label_map_all = {\n",
    "        'basophil': 0,\n",
    "        'eosinophil': 1,\n",
    "        'erythroblast': 2,\n",
    "        'myeloblast' : 3,\n",
    "        'promyelocyte': 4,\n",
    "        'myelocyte': 5,\n",
    "        'metamyelocyte': 6,\n",
    "        'neutrophil_banded': 7,\n",
    "        'neutrophil_segmented': 8,\n",
    "        'monocyte': 9,\n",
    "        'lymphocyte_typical': 10\n",
    "    }\n",
    "\n",
    "label_map_reverse = {\n",
    "        0: 'basophil',\n",
    "        1: 'eosinophil',\n",
    "        2: 'erythroblast',\n",
    "        3: 'myeloblast',\n",
    "        4: 'promyelocyte',\n",
    "        5: 'myelocyte',\n",
    "        6: 'metamyelocyte',\n",
    "        7: 'neutrophil_banded',\n",
    "        8: 'neutrophil_segmented',\n",
    "        9: 'monocyte',\n",
    "        10: 'lymphocyte_typical'\n",
    "    }\n",
    "\n",
    "# The unlabeled WBC dataset gets the classname 'Data-Val' for every image\n",
    "\n",
    "label_map_pred = {\n",
    "        'DATA-VAL': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> convert the dataset to a Pandas frame and compute the mean </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import ntpath\n",
    "import os\n",
    "import skimage.io as io\n",
    "savepaths=['metadata.csv', 'metadata_noisy.csv', 'metadata_rescaled.csv'] # path where the created dataframe will be stored\n",
    "savepath = savepaths[0]  # path where the created dataframe will be stored\n",
    "\n",
    "def finding_classes(data_dir):\n",
    "    \"\"\"\n",
    "    this function finds the folders in the root path and considers them\n",
    "    as classes\n",
    "    \"\"\"\n",
    "    classes = [folder for folder in sorted(os.listdir(data_dir)) if not folder.startswith('.') and not folder.startswith('_')]\n",
    "    return classes\n",
    "\n",
    "\n",
    "def metadata_generator(data_path):\n",
    "    #this function generates a pandas dataframe containing image information (paths, labels, dataset)\n",
    "    metadata = pd.DataFrame(columns=[\"Image\", \"file\", \"label\", \"dataset\", \"set\"])\n",
    "    for ds in data_path:\n",
    "        list_of_classes = finding_classes(data_path[ds])\n",
    "        for cl in list_of_classes:\n",
    "            metadata_dummy = pd.DataFrame(columns=[\"Image\", \"file\", \"label\", \"dataset\", \"set\", 'mean1', 'mean2', 'mean3'])\n",
    "            metadata_dummy[\"Image\"] = None\n",
    "            metadata_dummy[\"file\"] =  io.imread_collection(os.path.join(data_path[ds], cl, \"*\")).files\n",
    "            metadata_dummy[\"label\"] = cl\n",
    "            metadata_dummy[\"dataset\"] = ds\n",
    "            metadata_dummy[\"set\"] = \"train\"\n",
    "            for i in range(len(metadata_dummy)):\n",
    "                metadata_dummy['Image'].loc[i]=ntpath.basename(metadata_dummy['file'][i])\n",
    "            metadata = metadata.append(metadata_dummy, ignore_index=True)\n",
    "            metadata_dummy = None\n",
    "            \n",
    "    return metadata\n",
    "\n",
    "metadata = metadata_generator(data_path)\n",
    "\n",
    "def compute_mean(dataframe=metadata, savepath=savepath, selected_channels=[0,1,2]):\n",
    "    for idx in tqdm(range(len(dataframe)), position=0, leave=True):\n",
    "        if dataframe.loc(idx, \"dataset\") != \"WBC1\":\n",
    "            h5_file_path = dataframe.loc[idx,\"file\"]\n",
    "            try:\n",
    "                image= io.imread(h5_file_path)[:,:,selected_channels]\n",
    "            except ValueError: \n",
    "                print(h5_file_path)\n",
    "                break\n",
    "            #image = rgb2hsv(image)\n",
    "            dataframe.loc[idx, 'mean1']= np.mean(image[:,:,0])\n",
    "            dataframe.loc[idx, 'mean2']= np.mean(image[:,:,1])\n",
    "            dataframe.loc[idx, 'mean3']= np.mean(image[:,:,2])\n",
    "    dataframe.to_csv(savepath, index=False)\n",
    "    print(f'The dataframe was saved to {savepath}')\n",
    "    print(dataframe)\n",
    "    return dataframe\n",
    "\n",
    "compute_mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> in parallel </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "if not ray.is_initialized():\n",
    "    ray.init()\n",
    "import pandas as pd \n",
    "import ntpath\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import skimage.io as io\n",
    "\n",
    "savepaths=['metadata.csv', 'metadata_noisy.csv', 'metadata_rescaled.csv'] # path where the created dataframe will be stored\n",
    "savepath = savepaths[0]  # path where the created dataframe will be stored\n",
    "def finding_classes(data_dir):\n",
    "    \"\"\"\n",
    "    this function finds the folders in the root path and considers them\n",
    "    as classes\n",
    "    \"\"\"\n",
    "    classes = [folder for folder in sorted(os.listdir(data_dir)) if not folder.startswith('.') and not folder.startswith('_')]\n",
    "    return classes\n",
    "classes = []\n",
    "data_key = []\n",
    "metadata = []\n",
    "\n",
    "@ray.remote\n",
    "def metadata_dummy_getter(ds = data_key, cl = classes):\n",
    "    meta = []\n",
    "    metadata_dummy = pd.DataFrame(columns=[\"Image\", \"file\", \"label\", \"dataset\", \"set\", 'mean1', 'mean2', 'mean3'])\n",
    "    metadata_dummy[\"Image\"] = None\n",
    "    metadata_dummy[\"file\"] = io.imread_collection(os.path.join(data_path[ds], cl, \"*\")).files\n",
    "    metadata_dummy[\"label\"] = cl\n",
    "    metadata_dummy[\"dataset\"] = ds\n",
    "    metadata_dummy[\"set\"] = \"train\"\n",
    "    for i in range(len(metadata_dummy)):\n",
    "        metadata_dummy['Image'].loc[i]=ntpath.basename(metadata_dummy['file'][i])\n",
    "        meta.append(metadata_dummy)\n",
    "    return meta\n",
    "\n",
    "@ray.remote\n",
    "def metadata_generator(ds = data_path):\n",
    "    #this function generates a pandas dataframe containing image information (paths, labels, dataset)\n",
    "    metadata = pd.DataFrame(columns=[\"Image\", \"file\", \"label\", \"dataset\", \"set\"])\n",
    "    list_of_classes = finding_classes(data_path[ds]) \n",
    "    list_of_pandas = pd.concat(metadata.append(ray.get([metadata_dummy_getter.remote('Ace_20',cl) for cl in list_of_classes])))\n",
    "    list_of_pandas.index = range(len(list_of_pandas))\n",
    "    return list_of_pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "RayTaskError(TypeError)",
     "evalue": "\u001b[36mray::metadata_generator()\u001b[39m (pid=190328, ip=131.169.183.87)\n  File \"/tmp/ipykernel_189666/2327920929.py\", line 43, in metadata_generator\n  File \"/beegfs/desy/user/hailudaw/anacon/envs/tor/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n    return func(*args, **kwargs)\n  File \"/beegfs/desy/user/hailudaw/anacon/envs/tor/lib/python3.8/site-packages/pandas/core/reshape/concat.py\", line 347, in concat\n    op = _Concatenator(\n  File \"/beegfs/desy/user/hailudaw/anacon/envs/tor/lib/python3.8/site-packages/pandas/core/reshape/concat.py\", line 382, in __init__\n    raise TypeError(\nTypeError: first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(TypeError)\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_189666/526927713.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     print(list_classes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# x = ray.get([(metadata_dummy_getter.remote('Ace_20',cl) for cl in list_classes)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetadata_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/beegfs/desy/user/hailudaw/anacon/envs/tor/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/beegfs/desy/user/hailudaw/anacon/envs/tor/lib/python3.8/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1807\u001b[0m                     \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_object_store_memory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayTaskError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1809\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_instanceof_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1810\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRayTaskError(TypeError)\u001b[0m: \u001b[36mray::metadata_generator()\u001b[39m (pid=190328, ip=131.169.183.87)\n  File \"/tmp/ipykernel_189666/2327920929.py\", line 43, in metadata_generator\n  File \"/beegfs/desy/user/hailudaw/anacon/envs/tor/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n    return func(*args, **kwargs)\n  File \"/beegfs/desy/user/hailudaw/anacon/envs/tor/lib/python3.8/site-packages/pandas/core/reshape/concat.py\", line 347, in concat\n    op = _Concatenator(\n  File \"/beegfs/desy/user/hailudaw/anacon/envs/tor/lib/python3.8/site-packages/pandas/core/reshape/concat.py\", line 382, in __init__\n    raise TypeError(\nTypeError: first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\""
     ]
    }
   ],
   "source": [
    "# for ds in data_path:\n",
    "#     list_classes = finding_classes(data_path[ds])\n",
    "#     print(list_classes)\n",
    "# x = ray.get([(metadata_dummy_getter.remote('Ace_20',cl) for cl in list_classes)])\n",
    "metadata = pd.concat(ray.get([metadata_generator.remote(path) for path in data_path]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>dataset</th>\n",
       "      <th>set</th>\n",
       "      <th>mean1</th>\n",
       "      <th>mean2</th>\n",
       "      <th>mean3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BA_47.jpg</td>\n",
       "      <td>/beegfs/desy/user/hailudaw/challenge/Datasets/...</td>\n",
       "      <td>basophil</td>\n",
       "      <td>Ace_20</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BA_580.jpg</td>\n",
       "      <td>/beegfs/desy/user/hailudaw/challenge/Datasets/...</td>\n",
       "      <td>basophil</td>\n",
       "      <td>Ace_20</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BA_1223.jpg</td>\n",
       "      <td>/beegfs/desy/user/hailudaw/challenge/Datasets/...</td>\n",
       "      <td>basophil</td>\n",
       "      <td>Ace_20</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BA_1581.jpg</td>\n",
       "      <td>/beegfs/desy/user/hailudaw/challenge/Datasets/...</td>\n",
       "      <td>basophil</td>\n",
       "      <td>Ace_20</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BA_2035.jpg</td>\n",
       "      <td>/beegfs/desy/user/hailudaw/challenge/Datasets/...</td>\n",
       "      <td>basophil</td>\n",
       "      <td>Ace_20</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>PMY_988901.jpg</td>\n",
       "      <td>/beegfs/desy/user/hailudaw/challenge/Datasets/...</td>\n",
       "      <td>promyelocyte</td>\n",
       "      <td>Ace_20</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>PMY_989352.jpg</td>\n",
       "      <td>/beegfs/desy/user/hailudaw/challenge/Datasets/...</td>\n",
       "      <td>promyelocyte</td>\n",
       "      <td>Ace_20</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>PMY_991745.jpg</td>\n",
       "      <td>/beegfs/desy/user/hailudaw/challenge/Datasets/...</td>\n",
       "      <td>promyelocyte</td>\n",
       "      <td>Ace_20</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>PMY_993959.jpg</td>\n",
       "      <td>/beegfs/desy/user/hailudaw/challenge/Datasets/...</td>\n",
       "      <td>promyelocyte</td>\n",
       "      <td>Ace_20</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>PMY_996725.jpg</td>\n",
       "      <td>/beegfs/desy/user/hailudaw/challenge/Datasets/...</td>\n",
       "      <td>promyelocyte</td>\n",
       "      <td>Ace_20</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14291 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Image                                               file  \\\n",
       "0         BA_47.jpg  /beegfs/desy/user/hailudaw/challenge/Datasets/...   \n",
       "1        BA_580.jpg  /beegfs/desy/user/hailudaw/challenge/Datasets/...   \n",
       "2       BA_1223.jpg  /beegfs/desy/user/hailudaw/challenge/Datasets/...   \n",
       "3       BA_1581.jpg  /beegfs/desy/user/hailudaw/challenge/Datasets/...   \n",
       "4       BA_2035.jpg  /beegfs/desy/user/hailudaw/challenge/Datasets/...   \n",
       "..              ...                                                ...   \n",
       "587  PMY_988901.jpg  /beegfs/desy/user/hailudaw/challenge/Datasets/...   \n",
       "588  PMY_989352.jpg  /beegfs/desy/user/hailudaw/challenge/Datasets/...   \n",
       "589  PMY_991745.jpg  /beegfs/desy/user/hailudaw/challenge/Datasets/...   \n",
       "590  PMY_993959.jpg  /beegfs/desy/user/hailudaw/challenge/Datasets/...   \n",
       "591  PMY_996725.jpg  /beegfs/desy/user/hailudaw/challenge/Datasets/...   \n",
       "\n",
       "            label dataset    set mean1 mean2 mean3  \n",
       "0        basophil  Ace_20  train   NaN   NaN   NaN  \n",
       "1        basophil  Ace_20  train   NaN   NaN   NaN  \n",
       "2        basophil  Ace_20  train   NaN   NaN   NaN  \n",
       "3        basophil  Ace_20  train   NaN   NaN   NaN  \n",
       "4        basophil  Ace_20  train   NaN   NaN   NaN  \n",
       "..            ...     ...    ...   ...   ...   ...  \n",
       "587  promyelocyte  Ace_20  train   NaN   NaN   NaN  \n",
       "588  promyelocyte  Ace_20  train   NaN   NaN   NaN  \n",
       "589  promyelocyte  Ace_20  train   NaN   NaN   NaN  \n",
       "590  promyelocyte  Ace_20  train   NaN   NaN   NaN  \n",
       "591  promyelocyte  Ace_20  train   NaN   NaN   NaN  \n",
       "\n",
       "[14291 rows x 8 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ']' does not match opening parenthesis '(' (833299430.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_189666/833299430.py\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    meta = pd.concat(ray.get([compute_mean_meta.remote(meta, idx) for idx in tqdm(range(len(meta)-1)]))\u001b[0m\n\u001b[0m                                                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ']' does not match opening parenthesis '('\n"
     ]
    }
   ],
   "source": [
    "@ray.remote\n",
    "def compute_mean_meta(meta = metadata, idx = 0):\n",
    "    # for idx in tqdm(range(len(meta)), position=0, leave=True):\n",
    "    if meta.loc[idx, \"dataset\"] != \"WBC1\":\n",
    "        h5_file_path = meta.loc[idx,\"file\"]\n",
    "        image = io.imread(h5_file_path)\n",
    "        #image = rgb2hsv(image)\n",
    "        meta.loc[idx, 'mean1']= np.mean(image[:,:,0])\n",
    "        meta.loc[idx, 'mean2']= np.mean(image[:,:,1])\n",
    "        meta.loc[idx, 'mean3']= np.mean(image[:,:,2])\n",
    "    return meta\n",
    "\n",
    "@ray.remote\n",
    "def compute_mean(meta = metadata):\n",
    "    meta = pd.concat(ray.get([compute_mean_meta.remote(meta, idx) for idx in tqdm(range(len(meta)-1)]))\n",
    "    return meta\n",
    "    \n",
    "# mean_meta = ray.get([compute_mean.remote(meta) for meta in tqdm(metadata[0:1])])\n",
    "    # print(meta)\n",
    "    # for idx in tqdm(range(len(meta)), position=0, leave=True):\n",
    "    #     h5_file_path = meta.loc[idx,\"file\"]\n",
    "    #     image = io.imread(h5_file_path)\n",
    "    #     #image = rgb2hsv(image)\n",
    "    #     meta.loc[idx, 'mean1']= np.mean(image[:,:,0])\n",
    "    #     meta.loc[idx, 'mean2']= np.mean(image[:,:,1])\n",
    "    #     meta.loc[idx, 'mean3']= np.mean(image[:,:,2])\n",
    "    #     savepath = savepath + '_' + str(metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Authors\n",
    "\n",
    "> Armin Gruber\n",
    "\n",
    "> Ali Boushehri\n",
    "\n",
    "> Christina Bukas\n",
    "\n",
    "> Dawit Hailu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3178f61136cc71cf7bfb960c3038ac41089286ea2dd8757f89d6514789c9199a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
