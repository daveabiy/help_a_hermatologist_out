{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Challenge: [Help a Hematologist out!](https://helmholtz-data-challenges.de/web/challenges/challenge-page/93/overview) \n",
    "*** \n",
    "<b> Group: </b> \n",
    "> #      $ BLAMAD $  \n",
    "<b> members </b> \n",
    "> Bashir K., \n",
    "> Lea G., \n",
    "> Ankita N., \n",
    "> Martin B., \n",
    "> Arnab M., \n",
    "> Dawit H. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](https://github.com/christinab12/Data-challenge-logo/blob/main/logo.jpg?raw=true)\n",
    "\n",
    "## Getting started\n",
    "\n",
    "\n",
    "This notebook is a short summary for getting started with the challenge ( found [here](https://helmholtz-data-challenges.de/web/challenges/challenge-page/93/overview)  ). Below you can find how to download the dataset and also the different labels along with exploring and analyzing the input and output data of the challenge, running a baseline model and creating a submission file to upload to the leaderboard.\n",
    "\n",
    "***\n",
    "\n",
    "<b>dataset:</b>\n",
    "\n",
    "Three datasets, each constituting a different domain, will be used for this challenge:\n",
    "> 1. The Acevedo_20 dataset with labels\n",
    "> 2. The Matek_19 dataset with labels\n",
    "> 3. The WBC dataset <b> without labels </b> (Used for domain adaptation and performance measurement)\n",
    "\n",
    "The Acevedo_20 and Matek_19 datasets are labeled and should be used to train the model for the domain generalization task.\n",
    "A small subpart of the WBC dataset, WBC1, will be downloadable from the beginning of the challenge. It is unlabeled and should be used for evaluation and domain adaptation techniques.\n",
    "\n",
    "A second similar subpart of the WBC dataset, WBC2, will become available for download during phase 2 of the challenge, i.e. on the last day, 24 hours before submissions close.\n",
    "\n",
    "***\n",
    "<b>Goal: </b> \n",
    "\n",
    "The challenge here is in transfer learning, <b> precisely domain generalization (DG) and domain adaptation (DA) </b> techniques. The focus lies on using deep neural networks to classify single white blood cell images obtained from peripheral blood smears.\n",
    "<b> Tthe goal of this challenge is to achieve a high performance, especially a high f1 macro score, on the WBC2 dataset. </b>\n",
    "\n",
    "***\n",
    "<b>Notes: </b>\n",
    "\n",
    "This challenge wants to motivate research in domain generalization and adaptation techniques:\n",
    "\n",
    "To make actual use of deep learning in the medical routine, it is important that the techniques can be used in realistic cases. If a peripheral blood smear is acquired from a patient and classified by a neural network, it is important that this works reliably. But the patientâ€™s blood smear might very likely vary compared to the image domains used as training data of the network, resulting in not trustable results. To overcome this obstacle and build robust domain-invariant classifiers research in domain generalization and adaptation is needed.\n",
    "\n",
    "***\n",
    "<b>f1_score: </b>\n",
    "[wikepedia](https://en.wikipedia.org/wiki/F-score)\n",
    "\n",
    "> sklearn.metrics.f1_score(y_true, y_pred, *, labels=None, pos_label=1,<b> average='macro' </b>, sample_weight=None, zero_division='warn')\n",
    "\n",
    "The formula can be see in [click here for the code](https://github.com/scikit-learn/scikit-learn/blob/36958fb24/sklearn/metrics/_classification.py#L1001) and is given as\n",
    "\n",
    "> <g> F1 = 2 * (precision * recall) / (precision + recall) </g>\n",
    "\n",
    "<img src=\"/beegfs/desy/user/hailudaw/blamad/figures/f1_score.PNG\" width=\"384\" height=\"681\">\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Donwloading the data </b>\n",
    "\n",
    "Uncomment the code below to download the dataset. Makesure you adjust the path according to where you download it\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget --user YraZEdrHytaCSza --password BgZL3j8DT4 https://hmgubox2.helmholtz-muenchen.de/public.php/webdav/Acevedo_20.zip -O Acevedo_20.zip #(230M) [application/zip]\n",
    "# !wget --user YraZEdrHytaCSza --password BgZL3j8DT4 https://hmgubox2.helmholtz-muenchen.de/public.php/webdav/Matek_19.zip -O Matek_19.zip #(5.7G) [application/zip]\n",
    "# !wget --user YraZEdrHytaCSza --password BgZL3j8DT4 https://hmgubox2.helmholtz-muenchen.de/public.php/webdav/WBC1.zip -O WBC1.zip #(357M) [application/zip]\n",
    "# !wget --user YraZEdrHytaCSza --password BgZL3j8DT4 https://hmgubox2.helmholtz-muenchen.de/public.php/webdav/val_dummy.csv -O val_dummy.csv #44834 (44K) [text/csv]\n",
    "# !wget --user YraZEdrHytaCSza --password BgZL3j8DT4 https://hmgubox2.helmholtz-muenchen.de/public.php/webdav/metadata2.csv -O metadata2.csv #2019059 (1.9M) [text/csv]\n",
    "# print('download complete') \n",
    "\n",
    "# import shutil\n",
    "\n",
    "# shutil.unpack_archive('Acevedo_20.zip', 'Datasets/Acevedo_20')\n",
    "# shutil.unpack_archive('Matek_19.zip', 'Datasets/Matek_19')\n",
    "# shutil.unpack_archive('WBC1.zip', 'Datasets/WBC1')\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> datapath </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "Dataset_path = os.getcwd() + '/Datasets'\n",
    "data_path = { 'Ace_20': Dataset_path + '/Acevedo_20',\n",
    "              'Mat_19': Dataset_path + '/Matek_19',\n",
    "              'WBC1': Dataset_path + '/WBC1'}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> labels </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common classes of the datasets and their labels: \n",
    "# Highly underrepresented classes like atypical lymphocytes and smudge cells were left out.\n",
    "\n",
    "label_map_all = {'basophi': 0, 'eosinophi': 1, 'erythroblast': 2, 'myeloblast' : 3, 'promyelocyte': 4, 'myelocyte': 5, 'metamyelocyte': 6, 'neutrophil_banded': 7, 'neutrophil_segmented': 8, 'monocyte': 9, 'lymphocyte_typical': 10}\n",
    "label_map_reverse = { 0: 'basophil', 1: 'eosinophil', 2: 'erythroblast', 3: 'myeloblast', 4: 'promyelocyte', 5: 'myelocyte', 6: 'metamyelocyte', 7: 'neutrophil_banded', 8: 'neutrophil_segmented', 9: 'monocyte', 10: 'lymphocyte_typical'}\n",
    "\n",
    "# The unlabeled WBC dataset gets the classname 'Data-Val' for every image\n",
    "label_map_pred = {'DATA-VAL': 0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_processing as dp \n",
    "'''includes the data processing functions like  finding_classes, metadata_generator, compute_mean, data_report, data_plot, crop (data_loader, data_augmentation, data_preprocessing, data_splitting, data_normalization, data_visualization,)'''\n",
    "import dataset_generator as dg\n",
    "'''includes the dataset generator functions like  dataset_generator, dataset_generator_pred'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels for the datapath Ace_20: \", dp.finding_classes(data_path['Ace_20']))\n",
    "\n",
    "metadata = dp.metadata_generator(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((metadata.values[:,1]).tolist()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = io.imread(metadata.values[:,1].tolist()[0])\n",
    "image2 = io.imread(metadata.values[:,1].tolist()[15000])\n",
    "print(image.shape)\n",
    "\n",
    "for ch in range(3):\n",
    "    plt.subplot(1,3,ch+1, title='Channel {}'.format(ch+1, image.shape), xticks=[], yticks=[])\n",
    "    plt.imshow(image[:,:,ch])\n",
    "    plt.plot(image[:,:,ch][:,image.shape[1]//2], color='gray', linewidth=0.5)\n",
    "    plt.plot(image[:,:,ch][:,image.shape[0]//2], color='gray', linewidth=0.5)\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# np.mean(image, axis=(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "for ch in range(3):\n",
    "    print('Channel {}:'.format(ch+1))\n",
    "    print('Mean: ', np.mean(image[:,:,ch]))\n",
    "    metadata.values[:,5+ch] = np.mean(image[:,:,ch])\n",
    "    print('Std: ', np.std(image[:,:,ch]))\n",
    "    print('Max: ', np.max(image[:,:,ch]))\n",
    "    print('Min: ', np.min(image[:,:,ch]))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added = pd.DataFrame(columns=[\"Image\", \"file\", \"label\", \"dataset\", \"set\", 'mean1', 'mean2', 'mean3', 'std1', 'std2', 'std3', 'max1', 'max2', 'max3', 'min1', 'min2', 'min3'])\n",
    "metadata = pd.concat([metadata, added], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> convert the dataset to a Pandas frame and compute the mean </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray \n",
    "if not ray.is_initialized():\n",
    "    ray.init()\n",
    "\n",
    "@ray.remote\n",
    "def make_stat(metadata = metadata, idx = 0):\n",
    "    added = metadata.values[idx]\n",
    "    image = io.imread(metadata.values[:,1].tolist()[idx])\n",
    "    for ch in range(3):\n",
    "        added[5+ch] = np.mean(image[:,:,ch])\n",
    "        added[8+ch] = np.std(image[:,:,ch])\n",
    "        added[11+ch] = np.max(image[:,:,ch])\n",
    "        added[14+ch] = np.min(image[:,:,ch])\n",
    "    return added\n",
    "\n",
    "meta_ray = ray.put(metadata)\n",
    "x =ray.get([make_stat.remote(ray.get(meta_ray), id) for id in tqdm.tqdm(range(len(metadata)))])\n",
    "metadata.values[0:len(metadata)-1] = x[0:len(metadata)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_ray = ray.put(metadata)\n",
    "x =ray.get([make_stat.remote(ray.get(meta_ray), id) for id in tqdm.tqdm(range(len(metadata)))])\n",
    "metadata.values[0:len(metadata)-1] = x[0:len(metadata)-1]\n",
    "metadata.to_csv('metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head(21)\n",
    "\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Authors\n",
    "\n",
    "> Armin Gruber\n",
    "\n",
    "> Ali Boushehri\n",
    "\n",
    "> Christina Bukas\n",
    "\n",
    "> Dawit Hailu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3178f61136cc71cf7bfb960c3038ac41089286ea2dd8757f89d6514789c9199a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
